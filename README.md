# Learning Visual Attention for Robotic Vision

### Abstract

Visual saliency maps represent the attention-grabbing area of an image based on
the human vision nerve system. Existing deep learning architectures used to predict
saliency maps from images face an underlying challenge of generalization. In this
study, we explore the use of image transformation techniques and deeper model
architectures, such as ConvNext, a mixture of CNNs and Transformers as feature
extractors as a way of making the salency map predictions of the DeepGazeIIE
model more generalizable. This is done with aim of developing a deep learning.

Learn more in [report](https://drive.google.com/drive/u/0/folders/1FlmtSYcd-l7DbgHO7L5uMOwwoQtNbXJE).


Contributors: Denis Musinguzi, Kevin Sebineza, Jeande Dieu Nyandwi, Muhammed Danso.
### Acknowledgments

This project is a part of Introduction to Deep Learning(11-785). We thank the course staffs(instructors and TAs) for supporting us over the whole semester. We also thank authors of the baseline models we used for open-sourcing their codes.


