{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wandb --q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils, models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "from os.path import join as pjoin\n",
    "import logging\n",
    "import math\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from typing import Type, Any, Callable, Union, List, Optional\n",
    "import wandb\n",
    "\n",
    "from torch.nn import CrossEntropyLoss, Dropout, Softmax, Linear, Conv2d, LayerNorm\n",
    "from torch.nn.modules.utils import _pair\n",
    "from scipy import ndimage"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the saliency map data\n",
    "!gdown --id 1PnO7szbdub1559LfjYHMy65EDC4VhJC8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unzip the data \n",
    "!mkdir '/content/map'\n",
    "!unzip -qo '/content/maps.zip' -d  '/content/map'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the images\n",
    "!gdown --id 1g8j-hTT-51IG1UFwP0xTGhLdgIUCW5e5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unzip the images\n",
    "!mkdir '/content/images'\n",
    "!unzip -qo '/content/images.zip' -d  '/content/images'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is required for the dataset class to reduce the size of the images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocess\n",
    "\n",
    "def preprocess_img(img_dir, channels=3):\n",
    "\n",
    "    if channels == 1:\n",
    "        img = cv2.imread(img_dir, 0)\n",
    "    elif channels == 3:\n",
    "        img = cv2.imread(img_dir)\n",
    "\n",
    "    shape_r = 288\n",
    "    shape_c = 384\n",
    "    img_padded = np.ones((shape_r, shape_c, channels), dtype=np.uint8)\n",
    "    if channels == 1:\n",
    "        img_padded = np.zeros((shape_r, shape_c), dtype=np.uint8)\n",
    "    original_shape = img.shape\n",
    "    rows_rate = original_shape[0] / shape_r\n",
    "    cols_rate = original_shape[1] / shape_c\n",
    "    if rows_rate > cols_rate:\n",
    "        new_cols = (original_shape[1] * shape_r) // original_shape[0]\n",
    "        img = cv2.resize(img, (new_cols, shape_r))\n",
    "        if new_cols > shape_c:\n",
    "            new_cols = shape_c\n",
    "        img_padded[:,\n",
    "        ((img_padded.shape[1] - new_cols) // 2):((img_padded.shape[1] - new_cols) // 2 + new_cols)] = img\n",
    "    else:\n",
    "        new_rows = (original_shape[0] * shape_c) // original_shape[1]\n",
    "        img = cv2.resize(img, (shape_c, new_rows))\n",
    "\n",
    "        if new_rows > shape_r:\n",
    "            new_rows = shape_r\n",
    "        img_padded[((img_padded.shape[0] - new_rows) // 2):((img_padded.shape[0] - new_rows) // 2 + new_rows),\n",
    "        :] = img\n",
    "\n",
    "    return img_padded\n",
    "\n",
    "def postprocess_img(pred, org_dir):\n",
    "    pred = np.array(pred)\n",
    "    org = cv2.imread(org_dir, 0)\n",
    "    shape_r = org.shape[0]\n",
    "    shape_c = org.shape[1]\n",
    "    predictions_shape = pred.shape\n",
    "\n",
    "    rows_rate = shape_r / predictions_shape[0]\n",
    "    cols_rate = shape_c / predictions_shape[1]\n",
    "\n",
    "    if rows_rate > cols_rate:\n",
    "        new_cols = (predictions_shape[1] * shape_r) // predictions_shape[0]\n",
    "        pred = cv2.resize(pred, (new_cols, shape_r))\n",
    "        img = pred[:, ((pred.shape[1] - shape_c) // 2):((pred.shape[1] - shape_c) // 2 + shape_c)]\n",
    "    else:\n",
    "        new_rows = (predictions_shape[0] * shape_c) // predictions_shape[1]\n",
    "        pred = cv2.resize(pred, (shape_c, new_rows))\n",
    "        img = pred[((pred.shape[0] - shape_r) // 2):((pred.shape[0] - shape_r) // 2 + shape_r), :]\n",
    "\n",
    "    return img\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    \"\"\"Load dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, ids, stimuli_dir, saliency_dir, fixation_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.ids = ids\n",
    "        self.stimuli_dir = stimuli_dir\n",
    "        self.saliency_dir = saliency_dir\n",
    "        self.fixation_dir = fixation_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        im_path = self.stimuli_dir + self.ids.iloc[idx, 0]\n",
    "        image = Image.open(im_path).convert('RGB')\n",
    "        img = np.array(image) / 255.\n",
    "        img = np.transpose(img, (2, 0, 1))\n",
    "        img = torch.from_numpy(img)\n",
    "        if self.transform:\n",
    "           img = self.transform(image)\n",
    "\n",
    "        smap_path = self.saliency_dir + self.ids.iloc[idx, 1]\n",
    "        saliency = Image.open(smap_path)\n",
    "\n",
    "        smap = np.expand_dims(np.array(saliency) / 255., axis=0)\n",
    "        smap = torch.from_numpy(smap)\n",
    "\n",
    "        fmap_path = self.fixation_dir + self.ids.iloc[idx, 2]\n",
    "        fixation = Image.open(fmap_path)\n",
    "\n",
    "        fmap = np.expand_dims(np.array(fixation) / 255., axis=0)\n",
    "        fmap = torch.from_numpy(fmap)\n",
    "\n",
    "        sample = {'image': img, 'saliency': smap, 'fixation': fmap}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset class to process the images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    \"\"\"Load dataset.\"\"\"\n",
    "\n",
    "    def __init__(self,stimuli_dir, saliency_dir,transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        # self.ids = ids\n",
    "        self.image_dir = stimuli_dir\n",
    "        self.annotation_dir = saliency_dir\n",
    "        # self.fixation_dir = fixation_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # This line of code returns a sorted list of full paths to each image in the directory\n",
    "        self.img_paths = list(map(lambda fname: os.path.join(self.image_dir, fname), sorted(os.listdir(self.image_dir))))\n",
    "\n",
    "        # Annotated images\n",
    "        self.annotation_paths = list(map(lambda fname: os.path.join(self.annotation_dir, fname), sorted(os.listdir(self.annotation_dir))))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        image   = preprocess_img(self.img_paths[idx])\n",
    "\n",
    "        # image   = Image.open(self.img_paths[idx]).convert('RGB')\n",
    "\n",
    "        img = np.array(image) / 255.\n",
    "        img = np.transpose(img, (2, 0, 1))\n",
    "        img = torch.from_numpy(img)\n",
    "\n",
    "        if self.transform:\n",
    "           img = self.transform(image)\n",
    "\n",
    "\n",
    "\n",
    "        # smap_path = self.saliency_dir + self.ids.iloc[idx, 1]\n",
    "        # saliency = Image.open(self.annotation_paths[idx])\n",
    "        saliency   = preprocess_img(self.annotation_paths[idx], channels=1)\n",
    "\n",
    "\n",
    "        smap = np.expand_dims(np.array(saliency) / 255., axis=0)\n",
    "        smap = torch.from_numpy(smap)\n",
    "\n",
    "        sample = {'image': img, 'saliency': smap}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the train and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = MyDataset(stimuli_dir= '/content/images/images/train', \n",
    "                      saliency_dir= '/content/map/train',\n",
    "                      transform=transforms.Compose([\n",
    "                      # transforms.Resize((384, 288)),\n",
    "                      transforms.ToTensor(),\n",
    "                      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                      ]))\n",
    "\n",
    "val_set = MyDataset(stimuli_dir= '/content/images/images/val', \n",
    "                      saliency_dir= '/content/map/val',\n",
    "                      transform=transforms.Compose([\n",
    "                      # transforms.Resize((384, 288)),\n",
    "                      transforms.ToTensor(),\n",
    "                      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                            ]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {'train':DataLoader(train_set, batch_size=4,shuffle=True, num_workers=4),\n",
    "               'val':DataLoader(val_set, batch_size=32,shuffle=False, num_workers=4)}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check for the dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in dataloaders['val']:\n",
    "    print(data['saliency'].shape)\n",
    "    # print(data['saliency'].shape)\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View some of the images from the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some of the images in the dataset together with their annotations\n",
    "r, c = [2,4]\n",
    "fig, ax = plt.subplots(r, c, figsize = (15, 15)) \n",
    "\n",
    "k = 0\n",
    "\n",
    "for data in dataloaders['train']:\n",
    "    x, y = data['image'], data['saliency']\n",
    "\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "\n",
    "    for i in range(r):\n",
    "        for j in range(0, c, 2):\n",
    "            img = x[k].numpy().transpose(1,2,0)\n",
    "            ann = y[k].numpy().transpose(1,2,0)\n",
    "            # gauss = z[k].numpy().transpose(1,2,0)\n",
    "            ax[i, j].imshow(img)\n",
    "            ax[i, j].axis(\"off\")\n",
    "            ax[i, j+1].imshow(ann)\n",
    "            ax[i, j+1].axis(\"off\")\n",
    "            # ax[i, j+2].imshow(gauss)\n",
    "            # ax[i, j+2].axis(\"off\")\n",
    "            k +=1\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utils for the loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaliencyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SaliencyLoss, self).__init__()\n",
    "\n",
    "    def forward(self, preds, labels, loss_type='cc'):\n",
    "        losses = []\n",
    "        if loss_type == 'cc':\n",
    "            for i in range(labels.shape[0]): # labels.shape[0] is batch size\n",
    "                loss = loss_CC(preds[i],labels[i])\n",
    "                losses.append(loss)\n",
    "\n",
    "        elif loss_type == 'kldiv':\n",
    "            for i in range(labels.shape[0]):\n",
    "                loss = loss_KLdiv(preds[i],labels[i])\n",
    "                losses.append(loss)\n",
    "\n",
    "        elif loss_type == 'sim':\n",
    "            for i in range(labels.shape[0]):\n",
    "                loss = loss_similarity(preds[i],labels[i])\n",
    "                losses.append(loss)\n",
    "\n",
    "        elif loss_type == 'nss':\n",
    "            for i in range(labels.shape[0]):\n",
    "                loss = loss_NSS(preds[i],labels[i])\n",
    "                losses.append(loss)\n",
    "            \n",
    "        return torch.stack(losses).mean(dim=0, keepdim=True)\n",
    "        \n",
    "        \n",
    "def loss_KLdiv(pred_map, gt_map):\n",
    "    eps = 2.2204e-16\n",
    "    pred_map = pred_map/torch.sum(pred_map)\n",
    "    gt_map = gt_map/torch.sum(gt_map)\n",
    "    div = torch.sum(torch.mul(gt_map, torch.log(eps + torch.div(gt_map,pred_map+eps))))\n",
    "    return div \n",
    "        \n",
    "    \n",
    "def loss_CC(pred_map,gt_map):\n",
    "    gt_map_ = (gt_map - torch.mean(gt_map))\n",
    "    pred_map_ = (pred_map - torch.mean(pred_map))\n",
    "    cc = torch.sum(torch.mul(gt_map_,pred_map_))/torch.sqrt(torch.sum(torch.mul(gt_map_,gt_map_))*torch.sum(torch.mul(pred_map_,pred_map_)))\n",
    "    return cc\n",
    "\n",
    "\n",
    "def loss_similarity(pred_map,gt_map):\n",
    "    gt_map = (gt_map - torch.min(gt_map))/(torch.max(gt_map)-torch.min(gt_map))\n",
    "    gt_map = gt_map/torch.sum(gt_map)\n",
    "    \n",
    "    pred_map = (pred_map - torch.min(pred_map))/(torch.max(pred_map)-torch.min(pred_map))\n",
    "    pred_map = pred_map/torch.sum(pred_map)\n",
    "    \n",
    "    diff = torch.min(gt_map,pred_map)\n",
    "    score = torch.sum(diff)\n",
    "    \n",
    "    return score\n",
    "    \n",
    "    \n",
    "def loss_NSS(pred_map,fix_map):\n",
    "    '''ground truth here is fixation map'''\n",
    "\n",
    "    pred_map_ = (pred_map - torch.mean(pred_map))/torch.std(pred_map)\n",
    "    mask = fix_map.gt(0)\n",
    "    score = torch.mean(torch.masked_select(pred_map_, mask))\n",
    "    return score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'batch_size': 16, # Increase this if your GPU can handle it\n",
    "    'lr': 1e-5,\n",
    "    'epochs': 1, # 10 epochs is recommended ONLY for the early submission - you will have to train for much longer typically.\n",
    "    # Include other parameters as needed.\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention module"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "ACT2FN = {\"gelu\": torch.nn.functional.gelu, \"relu\": torch.nn.functional.relu}\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Attention, self).__init__()\n",
    "        self.num_attention_heads = config[\"num_heads\"]  # 12\n",
    "        self.attention_head_size = int(config['hidden_size'] / self.num_attention_heads)    # 42\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size    # 12*42=504\n",
    "\n",
    "        self.query = Linear(config['hidden_size'], self.all_head_size)  # (512, 504)\n",
    "        self.key = Linear(config['hidden_size'], self.all_head_size)\n",
    "        self.value = Linear(config['hidden_size'], self.all_head_size)\n",
    "\n",
    "        # self.out = Linear(config['hidden_size'], config['hidden_size'])\n",
    "        self.out = Linear(self.all_head_size, config['hidden_size'])\n",
    "        self.attn_dropout = Dropout(config[\"attention_dropout_rate\"])\n",
    "        self.proj_dropout = Dropout(config[\"attention_dropout_rate\"])\n",
    "\n",
    "        self.softmax = Softmax(dim=-1)\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "        mixed_key_layer = self.key(hidden_states)\n",
    "        mixed_value_layer = self.value(hidden_states)\n",
    "\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
    "\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        attention_probs = self.softmax(attention_scores)\n",
    "        attention_probs = self.attn_dropout(attention_probs)\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "        attention_output = self.out(context_layer)\n",
    "        attention_output = self.proj_dropout(attention_output)\n",
    "        return attention_output\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Mlp, self).__init__()\n",
    "        self.fc1 = Linear(config['hidden_size'], config[\"mlp_dim\"])\n",
    "        self.fc2 = Linear(config[\"mlp_dim\"], config['hidden_size'])\n",
    "        self.act_fn = ACT2FN[\"gelu\"]\n",
    "        self.dropout = Dropout(config[\"dropout_rate\"])\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        nn.init.normal_(self.fc1.bias, std=1e-6)\n",
    "        nn.init.normal_(self.fc2.bias, std=1e-6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Block, self).__init__()\n",
    "        self.flag = config['num_heads']\n",
    "        self.hidden_size = config['hidden_size']\n",
    "        self.ffn_norm = LayerNorm(config['hidden_size'], eps=1e-6)\n",
    "        self.ffn = Mlp(config)\n",
    "        self.attn = Attention(config)\n",
    "        self.attention_norm = LayerNorm(config['hidden_size'], eps=1e-6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = x\n",
    "\n",
    "        x = self.attention_norm(x)\n",
    "        x = self.attn(x)\n",
    "        x = x + h\n",
    "\n",
    "        h = x\n",
    "        x = self.ffn_norm(x)\n",
    "        x = self.ffn(x)\n",
    "        x = x + h\n",
    "        return x\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.layer = nn.ModuleList()\n",
    "        self.encoder_norm = LayerNorm(config['hidden_size'], eps=1e-6)\n",
    "        for _ in range(config[\"num_layers\"]):\n",
    "            layer = Block(config)\n",
    "            self.layer.append(copy.deepcopy(layer))\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        for layer_block in self.layer:\n",
    "            hidden_states = layer_block(hidden_states)\n",
    "        encoded = self.encoder_norm(hidden_states)\n",
    "\n",
    "        return encoded"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "efficient_model = torch.hub.load(\"pytorch/vision\", \"efficientnet_v2_l\", weights=\"IMAGENET1K_V1\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg1 = {\n",
    "\"hidden_size\" : 768,\n",
    "\"mlp_dim\" : 768*4,\n",
    "\"num_heads\" : 12,\n",
    "\"num_layers\" : 2,\n",
    "\"attention_dropout_rate\" : 0,\n",
    "\"dropout_rate\" : 0.0,\n",
    "}\n",
    "\n",
    "cfg2 = {\n",
    "\"hidden_size\" : 768,\n",
    "\"mlp_dim\" : 768*4,\n",
    "\"num_heads\" : 12,\n",
    "\"num_layers\" : 2,\n",
    "\"attention_dropout_rate\" : 0,\n",
    "\"dropout_rate\" : 0.0,\n",
    "}\n",
    "\n",
    "cfg3 = {\n",
    "\"hidden_size\" : 512,\n",
    "\"mlp_dim\" : 512*4,\n",
    "\"num_heads\" : 8,\n",
    "\"num_layers\" : 2,\n",
    "\"attention_dropout_rate\" : 0,\n",
    "\"dropout_rate\" : 0.0,\n",
    "}\n",
    "\n",
    "\n",
    "class TranSalNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(TranSalNet, self).__init__()\n",
    "        self.encoder = _Encoder()\n",
    "        self.decoder = _Decoder()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class _Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(_Encoder, self).__init__()\n",
    "        base_model = efficient_model #densenet201(pretrained=False)#161\n",
    "        base_layers = list(base_model.children())[0][:-1]\n",
    "        self.encoder = nn.ModuleList(base_layers).eval()\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = []\n",
    "        for ii,layer in enumerate(self.encoder):\n",
    "            x = layer(x)\n",
    "            if ii in {3, 5, 7}: #3\t5\t7\n",
    "                outputs.append(x)\n",
    "        return outputs\n",
    "\n",
    "class _Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(_Decoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.conv2 = nn.Conv2d(768, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.conv3 = nn.Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.conv4 = nn.Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.conv5 = nn.Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.conv6 = nn.Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.conv7 = nn.Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "\n",
    "        self.batchnorm1 = nn.BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.batchnorm3 = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.batchnorm4 = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.batchnorm5 = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.batchnorm6 = nn.BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "\n",
    "        #changed these to match the configs of efficientnet_v2_l\n",
    "        self.TransEncoder1 = TransEncoder(in_channels=640, spatial_size=9*12, cfg=cfg1)#<-2208\n",
    "        self.TransEncoder2 = TransEncoder(in_channels=224, spatial_size=18*24, cfg=cfg2)#<-2112\n",
    "        self.TransEncoder3 = TransEncoder(in_channels=96, spatial_size=36*48, cfg=cfg3)#<-768\n",
    "\n",
    "        self.add = torch.add\n",
    "        self.relu = nn.ReLU(True)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x3, x4, x5 = x\n",
    "\n",
    "        x5 = self.TransEncoder1(x5)\n",
    "        x5 = self.conv1(x5)\n",
    "        x5 = self.batchnorm1(x5)\n",
    "        x5 = self.relu(x5)\n",
    "        x5 = self.upsample(x5)\n",
    "\n",
    "        x4_a = self.TransEncoder2(x4)\n",
    "        x4 = x5 * x4_a\n",
    "        x4 = self.relu(x4)\n",
    "        x4 = self.conv2(x4)\n",
    "        x4 = self.batchnorm2(x4)\n",
    "        x4 = self.relu(x4)\n",
    "        x4 = self.upsample(x4)\n",
    "\n",
    "        x3_a = self.TransEncoder3(x3)\n",
    "        x3 = x4 * x3_a\n",
    "        x3 = self.relu(x3)\n",
    "        x3 = self.conv3(x3)\n",
    "        x3 = self.batchnorm3(x3)\n",
    "        x3 = self.relu(x3)\n",
    "        x3 = self.upsample(x3)\n",
    "\n",
    "        x2 = self.conv4(x3)\n",
    "        x2 = self.batchnorm4(x2)\n",
    "        x2 = self.relu(x2)\n",
    "        x2 = self.upsample(x2)\n",
    "        x2 = self.conv5(x2)\n",
    "        x2 = self.batchnorm5(x2)\n",
    "        x2 = self.relu(x2)\n",
    "\n",
    "        x1 = self.upsample(x2)\n",
    "        x1 = self.conv6(x1)\n",
    "        x1 = self.batchnorm6(x1)\n",
    "        x1 = self.relu(x1)\n",
    "        x1 = self.conv7(x1)\n",
    "        x = self.sigmoid(x1)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, spatial_size, cfg):\n",
    "        super(TransEncoder, self).__init__()\n",
    "\n",
    "        self.patch_embeddings = nn.Conv2d(in_channels=in_channels,\n",
    "                                          out_channels=cfg['hidden_size'],\n",
    "                                          kernel_size=1,\n",
    "                                          stride=1)\n",
    "        self.position_embeddings = nn.Parameter(torch.zeros(1, spatial_size, cfg['hidden_size']))\n",
    "\n",
    "        self.transformer_encoder = Encoder(cfg)\n",
    "\n",
    "    def forward(self, x):\n",
    "        a, b = x.shape[2], x.shape[3]\n",
    "        x = self.patch_embeddings(x)\n",
    "        x = x.flatten(2)\n",
    "        x = x.transpose(-1, -2)\n",
    "\n",
    "        embeddings = x + self.position_embeddings\n",
    "        x = self.transformer_encoder(embeddings)\n",
    "        B, n_patch, hidden = x.shape\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = x.contiguous().view(B, hidden, a, b)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transalnet_efficientnet = TranSalNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creata a dummy image\n",
    "myimage  = torch.randn(1,3,384, 288)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "z = transalnet_efficientnet(x)\n",
    "z.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def train(model, optimizer, criterion):\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    epoch_loss = 0\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    batch_bar   = tqdm(total=len(dataloaders['train']), dynamic_ncols=True, leave=False, position=0, desc='Train')\n",
    "\n",
    "    for i_batch, sample_batched in tqdm(enumerate(dataloaders['train'])):\n",
    "        # Initialize Gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        stimuli, smap = sample_batched['image'].to(device), sample_batched['saliency'].to(device)\n",
    "\n",
    "        with autocast(dtype= torch.float16):\n",
    "            outputs = model(stimuli)\n",
    "\n",
    "            loss = criterion(outputs, smap)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        scaler.step(optimizer)\n",
    "\n",
    "        # Updates the scale for next iteration.\n",
    "        scaler.update()\n",
    "\n",
    "        loss = loss.item()\n",
    "        epoch_loss += loss\n",
    "\n",
    "        batch_bar.set_postfix(loss=\"{:.04f}\".format(float(epoch_loss / (i_batch + 1))), lr=\"{:.04f}\".format(optimizer.param_groups[0]['lr']))\n",
    "        batch_bar.update()\n",
    "    \n",
    "    \n",
    "    batch_bar.close()\n",
    "\n",
    "    epoch_loss /= epoch_loss / (len(dataloaders['train']))\n",
    "    # self.epochs += 1\n",
    "    # print('[TRAIN] \\tEpoch [%d/%d] \\tLoss: %.4f \\tLr: %.6f'\n",
    "    #               % (self.epochs, self.max_epochs, epoch_loss, self.optimizer.param_groups[0]['lr']))\n",
    "    train_losses.append(epoch_loss)\n",
    "\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Store the validation losses\n",
    "validation_losses = SaliencyLoss()\n",
    "kl_div_list = list()\n",
    "sim_list    = list()\n",
    "cc_list     = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, dataloader):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    vloss1, vloss2, vloss3 = 0, 0, 0\n",
    "    batch_bar   = tqdm(total=len(dataloader), dynamic_ncols=True, leave=True, position=0, desc='Validation')\n",
    "    # batch_bar     = tqdm(dataloader)\n",
    "\n",
    "    for i_batch, sample_batched in tqdm(enumerate(dataloader)):\n",
    "        stimuli, smap = sample_batched['image'].to(device), sample_batched['saliency'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(stimuli)\n",
    "\n",
    "            loss1 = validation_losses(outputs, smap, 'cc')\n",
    "            loss2 = validation_losses(outputs, smap, 'kldiv')\n",
    "            loss3 = validation_losses(outputs, smap, 'sim')\n",
    "\n",
    "        vloss1  += loss1\n",
    "        vloss2  += loss2\n",
    "        vloss3  += loss3\n",
    "\n",
    "        batch_bar.set_postfix(cc=\"{:.04f}\".format(float(vloss1 / (i_batch + 1))), \n",
    "                              kldiv=\"{:.04f}\".format(float(vloss2 / (i_batch + 1))),\n",
    "                              sim=\"{:.04f}\".format(float(vloss3 / (i_batch + 1))))\n",
    "        batch_bar.update()\n",
    "\n",
    "    batch_bar.close()\n",
    "\n",
    "    vloss1   /= len(dataloader)\n",
    "    vloss2    /= len(dataloader)\n",
    "    vloss3    /= len(dataloader)\n",
    "\n",
    "    kl_div_list.append(vloss2)\n",
    "    sim_list.append(vloss3)\n",
    "    cc_list.append(vloss1)\n",
    "\n",
    "    return vloss1, vloss2, vloss3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer loss and scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(transalnet_efficientnet.parameters(),lr=1e-5)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "criterion   = torch.nn.BCEWithLogitsLoss()\n",
    "num_epochs = 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login(key=\"\")#Fill in the wandb key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your wandb run\n",
    "run = wandb.init(\n",
    "    name = \"transalnet_efficientnet\", ## Wandb creates random run names if you skip this field\n",
    "    reinit = True, ### Allows reinitalizing runs when you re-run this cell\n",
    "    # run_id = ### Insert specific run id here if you want to resume a previous run\n",
    "    # resume = \"must\" ### You need this to resume previous runs, but comment out reinit = True when using this\n",
    "    project = \"visual_saliency\", ### Project should be created in your wandb account \n",
    "    config = config ### Wandb Config for your run\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_vloss = 0.0\n",
    "\n",
    "for epoch in range(config['epochs']):\n",
    "\n",
    "    curr_lr = float(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "    train_loss = train(transalnet_efficientnet, optimizer, criterion)\n",
    "\n",
    "    print(\"\\nEpoch {}/{}: \\nTrain Loss {:.04f}\\t Learning Rate {:.04f}\".format(\n",
    "        epoch + 1,\n",
    "        config['epochs'],\n",
    "        train_loss,\n",
    "        curr_lr))\n",
    "    \n",
    "\n",
    "    if (epoch+1)%5 ==0:\n",
    "        vloss1, vloss2, vloss3 = eval(transalnet_efficientnet, dataloaders['val'])\n",
    "\n",
    "        # print(\"\\nEpoch {}/{}: \\n cc Loss {:.04f}\\t  sim Loss {:.04f} \\t kld Loss {:.04f}\\t Learning Rate {:.04f}\".format(\n",
    "        # epoch + 1,\n",
    "        # config['epochs'],\n",
    "        # vloss1,\n",
    "        # vloss3,\n",
    "        # vloss2,\n",
    "        # curr_lr))\n",
    "\n",
    "        wandb.log({\"train_loss\":train_loss, 'cc loss': vloss1, 'sim loss':vloss3, \n",
    "               'kld loss': vloss2, \"learning_Rate\": curr_lr})\n",
    "\n",
    "    scheduler.step()\n",
    "        \n",
    "run.finish()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
